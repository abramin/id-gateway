# Hot Path Analysis & Async Refactoring Opportunities

**Date:** 2026-01-05
**Scope:** `/authorize`, `/token`, session validation, revoke/logout
**Goal:** Identify synchronous I/O bottlenecks and propose DB outbox + Redis caching refactors

---

## Executive Summary

Credo's auth hot paths are **well-architected** with existing async audit via tri-publisher + outbox pattern. However, there are **5 targeted improvements** that can reduce p99 latency by ~20-40ms and improve resilience during downstream failures.

**Key Findings:**

1. Client resolution cache is in-memory only (not distributed across instances)
2. User lookup is not cached, causing repeated DB hits
3. Session store uses DB by default (Redis optional, underutilized)
4. Metrics recording is synchronous but lightweight (acceptable)
5. Audit publishing already uses outbox pattern (no changes needed)

**PRD Coverage:** Recommendations 1 and 2 have been added to [PRD-028](../prd/PRD-028-Performance-Optimization.md) as FR-7 and FR-8.

---

## A. Hot Path Inventory

| Endpoint              | Method | Handler                                                 | Service                                                                 | Rate Limit              |
| --------------------- | ------ | ------------------------------------------------------- | ----------------------------------------------------------------------- | ----------------------- |
| `/auth/authorize`     | POST   | [handler.go:86](internal/auth/handler/handler.go#L86)   | [authorize.go:44](internal/auth/service/authorize.go#L44)               | ClassAuth (10/min)      |
| `/auth/token`         | POST   | [handler.go:146](internal/auth/handler/handler.go#L146) | [token.go:17](internal/auth/service/token.go#L17)                       | ClassAuth (10/min)      |
| `/auth/userinfo`      | GET    | [handler.go:193](internal/auth/handler/handler.go#L193) | [userinfo.go:18](internal/auth/service/userinfo.go#L18)                 | ClassRead (100/min)     |
| `/auth/revoke`        | POST   | [handler.go:430](internal/auth/handler/handler.go#L430) | [token_revocation.go:29](internal/auth/service/token_revocation.go#L29) | ClassAuth (10/min)      |
| `/auth/sessions/{id}` | DELETE | [handler.go:252](internal/auth/handler/handler.go#L252) | [session_revoke.go:18](internal/auth/service/session_revoke.go#L18)     | ClassSensitive (30/min) |
| `/auth/logout-all`    | POST   | [handler.go:291](internal/auth/handler/handler.go#L291) | [session_revoke.go:95](internal/auth/service/session_revoke.go#L95)     | ClassSensitive (30/min) |

---

## B. Synchronous I/O Map by Hot Path

### B.1 POST /authorize

**Call Chain:**

```
HandleAuthorize
  → Service.Authorize()
    → clientResolver.ResolveClient()     [2 DB queries: client + tenant]
    → tx.RunInTx()
      → users.FindOrCreateByTenantAndEmail()  [1-2 DB ops]
      → sessions.Create()                      [1 DB INSERT]
      → codes.Create()                         [1 DB INSERT]
    → logAudit() [async via security publisher]
```

| I/O Type              | Count | Location                                                                | Blocking?               |
| --------------------- | ----- | ----------------------------------------------------------------------- | ----------------------- |
| DB Read (client)      | 1     | [client_service.go:499](internal/tenant/service/client_service.go#L499) | Yes                     |
| DB Read (tenant)      | 1     | [client_service.go:505](internal/tenant/service/client_service.go#L505) | Yes                     |
| DB Upsert (user)      | 1-2   | [store.go (FindOrCreate)](internal/auth/store/user/store.go)            | Yes                     |
| DB Insert (session)   | 1     | [store.go (Create)](internal/auth/store/session/store.go)               | Yes                     |
| DB Insert (auth code) | 1     | [store.go (Create)](internal/auth/store/authcode/store.go)              | Yes                     |
| Audit publish         | 1     | [observability.go:45](internal/auth/service/observability.go#L45)       | **No** (async buffered) |
| Metrics               | 3     | [authorize.go:45-48](internal/auth/service/authorize.go#L45)            | Yes (cheap)             |

**Total blocking DB ops: 5-6 per request**

---

### B.2 POST /token (Authorization Code Exchange)

**Call Chain:**

```
HandleToken
  → Service.Token()
    → exchangeAuthorizationCode()
      → codes.FindByCode()                [1 DB read]
      → sessions.FindByID()               [1 DB read]
      → clientResolver.ResolveClient()    [2 DB queries: client + tenant]
      → users.FindByID()                  [1 DB read]
      → jwt.GenerateAccessTokenWithJTI()  [CPU - before tx]
      → jwt.GenerateIDToken()             [CPU - before tx]
      → tx.RunInTx()
        → codes.Execute() [SELECT FOR UPDATE + UPDATE]
        → sessions.Execute() [SELECT FOR UPDATE + UPDATE]
        → refreshTokens.Create() [1 DB INSERT]
      → logAudit() [async]
```

| I/O Type                  | Count | Location                                                                | Blocking? |
| ------------------------- | ----- | ----------------------------------------------------------------------- | --------- |
| DB Read (auth code)       | 1     | [store.go (FindByCode)](internal/auth/store/authcode/store.go)          | Yes       |
| DB Read (session)         | 1     | [store.go (FindByID)](internal/auth/store/session/store.go)             | Yes       |
| DB Read (client)          | 1     | [client_service.go:499](internal/tenant/service/client_service.go#L499) | Yes       |
| DB Read (tenant)          | 1     | [client_service.go:505](internal/tenant/service/client_service.go#L505) | Yes       |
| DB Read (user)            | 1     | [store.go (FindByID)](internal/auth/store/user/store.go)                | Yes       |
| DB Lock+Update (code)     | 1     | [store.go (Execute)](internal/auth/store/authcode/store.go)             | Yes       |
| DB Lock+Update (session)  | 1     | [store.go (Execute)](internal/auth/store/session/store.go)              | Yes       |
| DB Insert (refresh token) | 1     | [store.go (Create)](internal/auth/store/refresh/store.go)               | Yes       |
| JWT Generation            | 2     | [token.go:100-101](internal/auth/service/token.go#L100)                 | Yes (CPU) |
| Audit publish             | 1     | [token_exchange.go:84](internal/auth/service/token_exchange.go#L84)     | **No**    |

**Total blocking DB ops: 8 per request**

---

### B.3 POST /token (Refresh Token Flow)

**Call Chain:**

```
HandleToken
  → Service.Token()
    → refreshWithRefreshToken()
      → refreshTokens.Find()              [1 DB read]
      → sessions.FindByID()               [1 DB read]
      → clientResolver.ResolveClient()    [2 DB queries]
      → users.FindByID()                  [1 DB read]
      → jwt.Generate...()                 [CPU - before tx]
      → tx.RunInTx()
        → refreshTokens.Execute() [SELECT FOR UPDATE + UPDATE]
        → sessions.Execute() [SELECT FOR UPDATE + UPDATE]
        → refreshTokens.Create() [1 DB INSERT]
      → logAudit() [async]
```

| I/O Type                 | Count | Location                                                                | Blocking? |
| ------------------------ | ----- | ----------------------------------------------------------------------- | --------- |
| DB Read (refresh token)  | 1     | [store.go (Find)](internal/auth/store/refresh/store.go)                 | Yes       |
| DB Read (session)        | 1     | [store.go (FindByID)](internal/auth/store/session/store.go)             | Yes       |
| DB Read (client)         | 1     | [client_service.go:499](internal/tenant/service/client_service.go#L499) | Yes       |
| DB Read (tenant)         | 1     | [client_service.go:505](internal/tenant/service/client_service.go#L505) | Yes       |
| DB Read (user)           | 1     | [store.go (FindByID)](internal/auth/store/user/store.go)                | Yes       |
| DB Lock+Update (token)   | 1     | [store.go (Execute)](internal/auth/store/refresh/store.go)              | Yes       |
| DB Lock+Update (session) | 1     | [store.go (Execute)](internal/auth/store/session/store.go)              | Yes       |
| DB Insert (new refresh)  | 1     | [store.go (Create)](internal/auth/store/refresh/store.go)               | Yes       |
| JWT Generation           | 2     | [token.go:100-101](internal/auth/service/token.go#L100)                 | Yes (CPU) |

**Total blocking DB ops: 8 per request**

---

### B.4 POST /revoke

**Call Chain:**

```
HandleRevoke
  → Service.RevokeToken()
    → jwt.ParseTokenSkipClaimsValidation() [CPU]
    → sessions.FindByID() or refreshTokens.Find() [1 DB read]
    → sessions.RevokeSessionIfActive() [SELECT FOR UPDATE + UPDATE]
    → trl.RevokeToken() [Redis SET or DB INSERT]
    → refreshTokens.DeleteBySessionID() [1 DB DELETE]
    → logAudit() [async]
```

| I/O Type                   | Count | Location                                                                      | Blocking? |
| -------------------------- | ----- | ----------------------------------------------------------------------------- | --------- |
| JWT Parse                  | 1     | [token_revocation.go:60](internal/auth/service/token_revocation.go#L60)       | Yes (CPU) |
| DB Read (session or token) | 1     | [token_revocation.go:100-120](internal/auth/service/token_revocation.go#L100) | Yes       |
| DB Lock+Update (session)   | 1     | [store.go (RevokeSessionIfActive)](internal/auth/store/session/store.go)      | Yes       |
| TRL Write (Redis/DB)       | 1     | [store_redis.go:24](internal/auth/store/revocation/store_redis.go#L24)        | Yes       |
| DB Delete (refresh tokens) | 1     | [store.go (DeleteBySessionID)](internal/auth/store/refresh/store.go)          | Yes       |

**Total blocking I/O: 4-5 per request**

---

## C. Audit Durability Assessment

### Current Architecture (GOOD)

```
Service Layer
    ↓
Tri-Publisher (Compliance | Security | Ops)
    ↓
Outbox Store (pkg/platform/audit/outbox/store/postgres/)
    ↓
Outbox Worker (pkg/platform/audit/outbox/worker/)
    ↓
Kafka (credo.audit.events topic)
    ↓
Audit Consumer → PostgreSQL Audit Store
```

**Files:**

- Outbox Entry: [outbox/models.go](pkg/platform/audit/outbox/models.go)
- Outbox Store: [outbox/store/postgres/store.go](pkg/platform/audit/outbox/store/postgres/store.go)
- Outbox Worker: [outbox/worker/worker.go](pkg/platform/audit/outbox/worker/worker.go)
- Kafka Producer: [kafka/producer/producer.go](internal/platform/kafka/producer/producer.go)

### Event Classification

| Event Type                     | Publisher  | Failure Mode               | Durability |
| ------------------------------ | ---------- | -------------------------- | ---------- |
| `user_created`, `user_deleted` | Security   | Async (buffered)           | DB outbox  |
| `consent_*`, `decision_made`   | Compliance | **Sync** (fail-closed)     | DB outbox  |
| `session_*`, `token_*`         | Security   | Async (buffered)           | DB outbox  |
| `auth_failed`                  | Security   | Async (buffered)           | DB outbox  |
| Operational events             | Ops        | Fire-and-forget + sampling | DB outbox  |

### Assessment: NO CHANGES NEEDED

The outbox pattern is already implemented correctly:

1. **Compliance events** are written to outbox in same transaction as business action
2. **Security events** use async buffered publishing with retry (10K buffer, 3 retries)
3. **Outbox worker** polls at 100ms intervals and publishes to Kafka
4. **Kafka failure** does not break auth - outbox entries persist in DB

**Metrics to monitor:**

- `credo_outbox_pending_total` > 1000 = warning
- `credo_audit_compliance_persist_failures_total` > 0 = critical

---

## D. Hot Path Slimming Analysis

### D.1 Work That MUST NOT Block /authorize and /token

| Work Type            | Current State                      | Recommendation                     |
| -------------------- | ---------------------------------- | ---------------------------------- |
| Audit events         | Already async (security publisher) | No change                          |
| Metrics recording    | Sync but ~10μs per call            | Acceptable                         |
| User lookup          | Sync DB read                       | **Cache in Redis**                 |
| Client/tenant lookup | In-memory cache (5min TTL)         | **Move to Redis for distribution** |
| Session writes       | Sync (required for correctness)    | No change                          |

### D.2 Candidates for Async Processing

**None identified.** The auth flows are correctly synchronous for correctness:

- Session creation must complete before returning auth code
- Token issuance must complete before returning tokens
- Audit is already async

---

## E. Redis Caching Opportunities

### E.1 Current Redis Usage

| Data                  | Key Pattern                         | TTL   | Status   |
| --------------------- | ----------------------------------- | ----- | -------- |
| Token Revocation List | `trl:jti:{jti}`                     | 15min | Active   |
| Sessions (optional)   | `session:{id}`                      | 24h   | Optional |
| User Session Sets     | `user_sessions:{id}`                | 25h   | Optional |
| Registry Cache        | `registry:citizen:{id}:{regulated}` | 5min  | Active   |
| Client Cache          | In-memory only                      | 5min  | **Gap**  |
| User Cache            | None                                | -     | **Gap**  |

### E.2 Recommended Additions

#### 1. Distributed Client Cache (HIGH PRIORITY)

**Problem:** [client_cache.go](internal/auth/adapters/client_cache.go) uses in-memory `sync.RWMutex` map. In multi-instance deployments, each pod has separate cache, causing:

- Cache miss storms on new pods
- Inconsistent cache state across instances
- Higher DB load during scaling events

**Solution:** Redis-backed client cache

```go
// Key pattern: client:{oauth_client_id}
// Value: JSON { client: {...}, tenant: {...} }
// TTL: 5 minutes (same as current in-memory)

// Invalidation: Write-through on client update (already in tenant service)
```

**Files to modify:**

- Create: `internal/auth/adapters/client_cache_redis.go`
- Modify: [resilient_client_resolver.go:83](internal/auth/adapters/resilient_client_resolver.go#L83)
- Modify: [cmd/server/main.go:499-505](cmd/server/main.go#L499) (inject Redis client)

**Impact:** Eliminate 2 DB queries per request when cache hit (cross-instance)

---

#### 2. User Lookup Cache (MEDIUM PRIORITY)

**Problem:** [token.go:58](internal/auth/service/token.go#L58) calls `users.FindByID()` on every token exchange/refresh. Users rarely change, but this lookup happens on every token operation.

**Solution:** Redis cache with short TTL

```go
// Key pattern: user:{user_id}
// Value: JSON serialized user (excluding sensitive fields)
// TTL: 60 seconds (short because user status can change)

// Invalidation:
// - Delete on user status change (deactivate, delete)
// - TTL handles eventual consistency
```

**Files to modify:**

- Create: `internal/auth/store/user/store_cached.go` (decorator pattern)
- Modify: [cmd/server/main.go](cmd/server/main.go) (wire cached store)

**Impact:** Eliminate 1 DB query per token operation (when cached)

---

#### 3. Session Read Cache (LOW PRIORITY - Already Exists)

**Current:** Sessions can use Redis store via [store_redis.go](internal/auth/store/session/store_redis.go)

**Gap:** Default config uses PostgreSQL. Consider:

- Primary storage in PostgreSQL (durability)
- Read-through cache in Redis (performance)

**Recommendation:** Add write-through cache layer if session reads are bottleneck (measure first).

---

### E.3 Cache Invalidation Strategy

| Cache         | Invalidation Trigger     | Strategy                                       |
| ------------- | ------------------------ | ---------------------------------------------- |
| Client cache  | Client update/deactivate | TTL (5min) + explicit delete on mutation       |
| User cache    | User deactivate/delete   | TTL (60s) + explicit delete on mutation        |
| Session cache | Session revoke           | Explicit delete (already handled by Redis TRL) |

**Critical:** For revocation/disable, use "delete on mutation + short TTL" pattern. Never rely solely on TTL for security-critical state.

---

## F. Prioritized Refactor List

> **Note:** Priority 1 and 2 are now tracked as [PRD-028 FR-7 and FR-8](../prd/PRD-028-Performance-Optimization.md#fr-7-distributed-client-resolution-cache-redis).

### Priority 1: Distributed Client Cache (HIGH ROI, LOW RISK) - PRD-028 FR-7

**Impact:** -20ms p99 latency on token operations (eliminates 2 DB queries)
**Risk:** Low (cache miss falls through to DB)
**Effort:** ~4 hours

**Implementation:**

1. Create `internal/auth/adapters/client_cache_redis.go`:

```go
type RedisClientCache struct {
    client *redis.Client
    ttl    time.Duration
}

func (c *RedisClientCache) Get(ctx context.Context, clientID string) (*types.ResolvedClient, *types.ResolvedTenant, bool) {
    key := "client:" + clientID
    data, err := c.client.Get(ctx, key).Bytes()
    if err == redis.Nil {
        return nil, nil, false
    }
    // unmarshal and return
}

func (c *RedisClientCache) Set(ctx context.Context, clientID string, client *types.ResolvedClient, tenant *types.ResolvedTenant) {
    key := "client:" + clientID
    data, _ := json.Marshal(clientCacheEntry{Client: client, Tenant: tenant})
    c.client.Set(ctx, key, data, c.ttl)
}
```

2. Modify [resilient_client_resolver.go](internal/auth/adapters/resilient_client_resolver.go):

   - Accept `RedisClientCache` as optional dependency
   - Fall back to in-memory cache if Redis unavailable

3. Wire in [main.go:499-505](cmd/server/main.go#L499)

**Test Plan:**

- Unit: Cache hit/miss behavior
- Integration: Redis failure fallback to DB
- E2E: Token exchange with Redis down

---

### Priority 2: User Lookup Cache (MEDIUM ROI, LOW RISK) - PRD-028 FR-8

**Impact:** -10ms p99 latency on token operations
**Risk:** Low (short TTL + explicit invalidation)
**Effort:** ~3 hours

**Implementation:**

1. Create `internal/auth/store/user/store_cached.go`:

```go
type CachedUserStore struct {
    delegate UserStore
    cache    *redis.Client
    ttl      time.Duration // 60s recommended
}

func (s *CachedUserStore) FindByID(ctx context.Context, userID id.UserID) (*models.User, error) {
    key := "user:" + userID.String()
    if cached, err := s.cache.Get(ctx, key).Bytes(); err == nil {
        var user models.User
        json.Unmarshal(cached, &user)
        return &user, nil
    }

    user, err := s.delegate.FindByID(ctx, userID)
    if err == nil {
        data, _ := json.Marshal(user)
        s.cache.Set(ctx, key, data, s.ttl)
    }
    return user, err
}

func (s *CachedUserStore) Delete(ctx context.Context, userID id.UserID) error {
    s.cache.Del(ctx, "user:" + userID.String()) // Invalidate cache
    return s.delegate.Delete(ctx, userID)
}
```

2. Wire in [main.go](cmd/server/main.go) when building auth module

**Test Plan:**

- Unit: Cache populate, hit, invalidate
- Integration: User deactivation clears cache
- Failure injection: Redis timeout (should fallback to DB)

---

### Priority 3: Consolidate Pre-Transaction Reads (MEDIUM ROI, NO NEW DEPS)

**Impact:** Reduce DB round-trips by reusing data already fetched
**Risk:** Very low (code reorganization only)
**Effort:** ~2 hours

**Current Issue:** In token flows, session is fetched twice:

1. First in `exchangeAuthorizationCode()` or `refreshWithRefreshToken()` (pre-tx read)
2. Again in `Execute()` callback (locked read)

The second read is necessary for locking, but some validation happens before the lock.

**Recommendation:** Document this as intentional (TOCTOU protection). The duplicate read is correct for correctness. No change needed.

---

### Priority 4: Metrics Batching (LOW ROI, LOW RISK)

**Current:** Metrics are recorded synchronously via Prometheus client. Each `Observe()` or `Inc()` takes ~1-10μs.

**Assessment:** Acceptable. Prometheus client is highly optimized. Total metrics overhead per request is <100μs.

**No change recommended** unless profiling shows otherwise.

---

### Priority 5: Session Store Redis Migration (CONDITIONAL)

**Current:** Sessions stored in PostgreSQL by default.

**Recommendation:** If session read latency becomes a bottleneck:

1. Enable Redis session store (already implemented in [store_redis.go](internal/auth/store/session/store_redis.go))
2. Use PostgreSQL as fallback for persistence

**Prerequisite:** Measure session read latency. Only proceed if p99 > 50ms.

---

## G. Test Plan for Refactors

### G.1 Integration Tests

Add to [e2e/features/](e2e/features/):

```gherkin
@resilience
Feature: Auth resilience under infrastructure failures

  Scenario: Token exchange succeeds when Redis is unavailable
    Given Redis is down
    When I exchange an authorization code for tokens
    Then the exchange succeeds with DB fallback
    And metrics show cache_miss

  Scenario: Token refresh succeeds with slow database
    Given database queries take 500ms
    When I refresh a token
    Then the refresh completes within 2 seconds
    And the client cache reduces DB calls

  Scenario: Client cache invalidation on deactivation
    Given a cached client
    When the client is deactivated via admin API
    Then subsequent token requests fail with invalid_client
```

### G.2 Failure Injection Tests

Add to integration test suite:

```go
// TestRedisFailureFallback verifies auth works when Redis is down
func TestRedisFailureFallback(t *testing.T) {
    // 1. Populate client cache in Redis
    // 2. Stop Redis container
    // 3. Attempt token exchange
    // 4. Verify success (DB fallback)
    // 5. Verify metrics incremented
}

// TestDatabaseSlowQuery verifies timeouts work correctly
func TestDatabaseSlowQuery(t *testing.T) {
    // 1. Add pg_sleep to queries via test hook
    // 2. Attempt authorize
    // 3. Verify timeout after 5s (transaction timeout)
    // 4. Verify no partial writes
}
```

---

## H. Summary: Changes by Priority

| #   | Change                | PRD          | Files                                       | Impact      | Effort | Risk |
| --- | --------------------- | ------------ | ------------------------------------------- | ----------- | ------ | ---- |
| 1   | Redis client cache    | PRD-028 FR-7 | `adapters/client_cache_redis.go`, `main.go` | -20ms p99   | 4h     | Low  |
| 2   | Redis user cache      | PRD-028 FR-8 | `store/user/store_cached.go`, `main.go`     | -10ms p99   | 3h     | Low  |
| 3   | Document pre-tx reads | N/A          | `token_exchange.go` (comments)              | Clarity     | 30m    | None |
| 4   | Metrics batching      | N/A          | None                                        | N/A         | 0      | N/A  |
| 5   | Redis sessions        | N/A          | Config change only                          | Conditional | 1h     | Low  |

**Total estimated impact:** 20-30ms p99 reduction on token operations.

---

## I. What NOT to Change

1. **Audit outbox pattern** - Already correct, provides durability
2. **Transaction mutex** - Necessary for in-memory store compatibility
3. **JWT generation timing** - Already optimized (before transaction)
4. **Metrics recording** - Already fast enough
5. **Rate limiting in DB** - Intentional design for consistency across instances
